{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import AdamW, get_scheduler\n\n\nargs = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", per_device_train_batch_size=16)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T09:58:44.420663Z","iopub.execute_input":"2024-05-20T09:58:44.421073Z","iopub.status.idle":"2024-05-20T09:59:10.745326Z","shell.execute_reply.started":"2024-05-20T09:58:44.421042Z","shell.execute_reply":"2024-05-20T09:59:10.744258Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-20 09:58:57.768979: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-20 09:58:57.769154: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-20 09:58:57.945306: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"! pip install transformers datasets\n! pip install evaluate\n! pip install sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-22T17:04:08.017118Z","iopub.execute_input":"2024-05-22T17:04:08.017698Z","iopub.status.idle":"2024-05-22T17:04:50.359417Z","shell.execute_reply.started":"2024-05-22T17:04:08.017657Z","shell.execute_reply":"2024-05-22T17:04:50.357445Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\nCollecting sentence-transformers\n  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.22.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\n\nimport datasets\nfrom datasets.tasks import TextClassification\n\n\n_DESCRIPTION = \"\"\"\\\nLarge Yelp Review Dataset.\nThis is a dataset for binary sentiment classification. \\\nWe provide a set of 560,000 highly polar yelp reviews for training, and 38,000 for testing. \\\n\nORIGIN\nThe Yelp reviews dataset consists of reviews from Yelp. It is extracted\nfrom the Yelp Dataset Challenge 2015 data. For more information, please\nrefer to http://www.yelp.com/dataset_challenge\n\nThe Yelp reviews polarity dataset is constructed by\nXiang Zhang (xiang.zhang@nyu.edu) from the above dataset.\nIt is first used as a text classification benchmark in the following paper:\nXiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks\nfor Text Classification. Advances in Neural Information Processing Systems 28\n(NIPS 2015).\n\n\nDESCRIPTION\n\nThe Yelp reviews polarity dataset is constructed by considering stars 1 and 2\nnegative, and 3 and 4 positive. For each polarity 280,000 training samples and\n19,000 testing samples are take randomly. In total there are 560,000 trainig\nsamples and 38,000 testing samples. Negative polarity is class 1,\nand positive class 2.\n\nThe files train.csv and test.csv contain all the training samples as\ncomma-sparated values. There are 2 columns in them, corresponding to class\nindex (1 and 2) and review text. The review texts are escaped using double\nquotes (\"), and any internal double quote is escaped by 2 double quotes (\"\").\nNew lines are escaped by a backslash followed with an \"n\" character,\nthat is \"\\n\".\n\"\"\"\n\n_CITATION = \"\"\"\\\n@article{zhangCharacterlevelConvolutionalNetworks2015,\n  archivePrefix = {arXiv},\n  eprinttype = {arxiv},\n  eprint = {1509.01626},\n  primaryClass = {cs},\n  title = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},\n  abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},\n  journal = {arXiv:1509.01626 [cs]},\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  month = sep,\n  year = {2015},\n}\n\n\"\"\"\n\n_DOWNLOAD_URL = \"https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz\"\n\n\nclass YelpPolarityReviewsConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for YelpPolarityReviews.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"BuilderConfig for YelpPolarityReviews.\n\n        Args:\n\n            **kwargs: keyword arguments forwarded to super.\n        \"\"\"\n        super(YelpPolarityReviewsConfig, self).__init__(version=datasets.Version(\"1.0.0\", \"\"), **kwargs)\n\n\nclass YelpPolarity(datasets.GeneratorBasedBuilder):\n    \"\"\"Yelp Polarity reviews dataset.\"\"\"\n\n    BUILDER_CONFIGS = [\n        YelpPolarityReviewsConfig(\n            name=\"plain_text\",\n            description=\"Plain text\",\n        )\n    ]\n\n    def _info(self):\n        return datasets.DatasetInfo(\n            description=_DESCRIPTION,\n            features=datasets.Features(\n                {\n                    \"text\": datasets.Value(\"string\"),\n                    \"label\": datasets.features.ClassLabel(names=[\"1\", \"2\"]),\n                }\n            ),\n            supervised_keys=None,\n            homepage=\"https://course.fast.ai/datasets\",\n            citation=_CITATION,\n            task_templates=[TextClassification(text_column=\"text\", label_column=\"label\")],\n        )\n\n    def _vocab_text_gen(self, train_file):\n        for _, ex in self._generate_examples(train_file):\n            yield ex[\"text\"]\n\n    def _split_generators(self, dl_manager):\n        arch_path = dl_manager.download(_DOWNLOAD_URL)\n        train_file = \"yelp_review_polarity_csv/train.csv\"\n        test_file = \"yelp_review_polarity_csv/test.csv\"\n        return [\n            datasets.SplitGenerator(\n                name=datasets.Split.TRAIN,\n                gen_kwargs={\n                    \"filepath\": train_file,\n                    \"files\": dl_manager.iter_archive(arch_path),\n                },\n            ),\n            datasets.SplitGenerator(\n                name=datasets.Split.TEST,\n                gen_kwargs={\n                    \"filepath\": test_file,\n                    \"files\": dl_manager.iter_archive(arch_path),\n                },\n            ),\n        ]\n\n    def _generate_examples(self, filepath, files):\n        \"\"\"Generate Yelp examples.\"\"\"\n        for path, f in files:\n            if path == filepath:\n                for line_id, line in enumerate(f):\n                    line = line.decode(\"utf-8\")\n                    # The format of the line is:\n                    # \"1\", \"The text of the review.\"\n                    yield line_id, {\"text\": line[5:-2].strip(), \"label\": line[1]}\n                break\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:02:53.268717Z","iopub.execute_input":"2024-05-20T10:02:53.269182Z","iopub.status.idle":"2024-05-20T10:02:53.303379Z","shell.execute_reply.started":"2024-05-20T10:02:53.269148Z","shell.execute_reply":"2024-05-20T10:02:53.300928Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"fancyzhx/yelp_polarity\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T17:06:00.910922Z","iopub.execute_input":"2024-05-22T17:06:00.911454Z","iopub.status.idle":"2024-05-22T17:06:02.979233Z","shell.execute_reply.started":"2024-05-22T17:06:00.911407Z","shell.execute_reply":"2024-05-22T17:06:02.977999Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:04:03.258589Z","iopub.execute_input":"2024-05-20T10:04:03.259156Z","iopub.status.idle":"2024-05-20T10:04:03.266920Z","shell.execute_reply.started":"2024-05-20T10:04:03.259111Z","shell.execute_reply":"2024-05-20T10:04:03.264920Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 560000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 38000\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"# insert your code here\ntrain_dataset = dataset['train'].select(range(300))\ntest_dataset= dataset['test'].select(range(300))\nprint(train_dataset)\nprint(\"\\n\")\nprint(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T17:06:10.242177Z","iopub.execute_input":"2024-05-22T17:06:10.243567Z","iopub.status.idle":"2024-05-22T17:06:10.260859Z","shell.execute_reply.started":"2024-05-22T17:06:10.243508Z","shell.execute_reply":"2024-05-22T17:06:10.259139Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['text', 'label'],\n    num_rows: 300\n})\n\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 300\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:04:37.141458Z","iopub.execute_input":"2024-05-20T10:04:37.141938Z","iopub.status.idle":"2024-05-20T10:04:37.151398Z","shell.execute_reply.started":"2024-05-20T10:04:37.141904Z","shell.execute_reply":"2024-05-20T10:04:37.150218Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'label'],\n    num_rows: 300\n})"},"metadata":{}}]},{"cell_type":"code","source":"from collections import Counter\n\ntrain_categories = Counter(train_dataset[\"label\"])\ntest_categories = Counter(test_dataset[\"label\"])\n\nprint(\"train set:\", len(train_categories))\nprint(\"test set:\", len(test_categories))\nprint(\"\\n\")\n\n# Υπολογισμός του αριθμού των δειγμάτων ανά κατηγορία στα σύνολα train και test\ntrain_samples_per_category = {category: min(count, 300) for category, count in train_categories.items()}\ntest_samples_per_category = {category: min(count, 300) for category, count in test_categories.items()}\n\nprint(\"Samples per category in train set:\", train_samples_per_category)\nprint(\"Samples per category in test set:\", test_samples_per_category)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:04:53.710973Z","iopub.execute_input":"2024-05-20T10:04:53.711487Z","iopub.status.idle":"2024-05-20T10:04:53.724703Z","shell.execute_reply.started":"2024-05-20T10:04:53.711435Z","shell.execute_reply":"2024-05-20T10:04:53.723049Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"train set: 2\ntest set: 2\n\n\nSamples per category in train set: {0: 203, 1: 97}\nSamples per category in test set: {1: 155, 0: 145}\n","output_type":"stream"}]},{"cell_type":"code","source":"# insert your code here\nfrom transformers import AutoTokenizer, BertForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\nmodel = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T17:06:15.851145Z","iopub.execute_input":"2024-05-22T17:06:15.853337Z","iopub.status.idle":"2024-05-22T17:06:21.181561Z","shell.execute_reply.started":"2024-05-22T17:06:15.853296Z","shell.execute_reply":"2024-05-22T17:06:21.180033Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02c60c3a5fb847b0b892c5a38a862a4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/520 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cf17d33b76a48a0bd40fbf56fc8679b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f420a32c0336413180edbe6c1b33cc6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cae93927411e4fa4acd301558d06ed05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f886852d0bb24771ad8970b5c596e284"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n# insert your code here\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\n\n# Προεπεξεργασία του test set\ntest_dataset = test_dataset.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T17:06:28.589892Z","iopub.execute_input":"2024-05-22T17:06:28.590551Z","iopub.status.idle":"2024-05-22T17:06:29.046379Z","shell.execute_reply.started":"2024-05-22T17:06:28.590512Z","shell.execute_reply":"2024-05-22T17:06:29.045062Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cfade51cf654c97bf887ebbdf621bd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4819ac9dc5f94a1c8785f4d0df4792dc"}},"metadata":{}}]},{"cell_type":"code","source":"test_dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:06:10.194026Z","iopub.execute_input":"2024-05-20T10:06:10.194546Z","iopub.status.idle":"2024-05-20T10:06:10.203948Z","shell.execute_reply.started":"2024-05-20T10:06:10.194498Z","shell.execute_reply":"2024-05-20T10:06:10.202231Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 300\n})"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:06:21.850981Z","iopub.execute_input":"2024-05-20T10:06:21.851481Z","iopub.status.idle":"2024-05-20T10:06:21.859767Z","shell.execute_reply.started":"2024-05-20T10:06:21.851432Z","shell.execute_reply":"2024-05-20T10:06:21.858517Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 300\n})"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport evaluate\nimport torch\nfrom tqdm import tqdm\nfrom transformers import pipeline\n\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T17:06:34.025815Z","iopub.execute_input":"2024-05-22T17:06:34.027107Z","iopub.status.idle":"2024-05-22T17:06:34.988950Z","shell.execute_reply.started":"2024-05-22T17:06:34.027014Z","shell.execute_reply":"2024-05-22T17:06:34.987731Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da44f9eaa60446d79d5a3ade007d4d22"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nfrom transformers import AdamW, get_scheduler\n\n\nargs = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", per_device_train_batch_size=16)\ntraining_args = TrainingArguments(\n    output_dir=\"test_trainer\",\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    weight_decay=0.01\n)\n\n# Ορισμός του optimizer\noptimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n\n# Ορισμός του scheduler\nnum_training_steps =5\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\n# Δημιουργία του Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    optimizers=(optimizer, lr_scheduler)\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:36:30.064040Z","iopub.execute_input":"2024-05-20T10:36:30.064690Z","iopub.status.idle":"2024-05-20T10:36:30.128056Z","shell.execute_reply.started":"2024-05-20T10:36:30.064645Z","shell.execute_reply":"2024-05-20T10:36:30.126378Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:36:37.927229Z","iopub.execute_input":"2024-05-20T10:36:37.927718Z","iopub.status.idle":"2024-05-20T10:36:37.950729Z","shell.execute_reply.started":"2024-05-20T10:36:37.927680Z","shell.execute_reply":"2024-05-20T10:36:37.949573Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"trained_model=trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:36:42.652950Z","iopub.execute_input":"2024-05-20T10:36:42.653810Z","iopub.status.idle":"2024-05-20T11:42:39.539765Z","shell.execute_reply.started":"2024-05-20T10:36:42.653761Z","shell.execute_reply":"2024-05-20T11:42:39.538171Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [57/57 1:05:03, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.122987</td>\n      <td>0.983333</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.118358</td>\n      <td>0.983333</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.115467</td>\n      <td>0.986667</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments\nfrom transformers import AdamW, get_scheduler\n\nargs=TrainingArguments(output_dir=\"test_trainer\",)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T16:57:42.021393Z","iopub.execute_input":"2024-05-22T16:57:42.021778Z","iopub.status.idle":"2024-05-22T16:57:49.476171Z","shell.execute_reply.started":"2024-05-22T16:57:42.021747Z","shell.execute_reply":"2024-05-22T16:57:49.474900Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nfrom transformers import AdamW, get_scheduler\n\n\nargs = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", per_device_train_batch_size=16)\ntraining_args = TrainingArguments(\n    output_dir=\"test_trainer\",\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=5,\n    weight_decay=0.01\n)\n\n# Ορισμός του optimizer\noptimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n\n# Ορισμός του scheduler\nnum_training_steps =5\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\n# Δημιουργία του Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    optimizers=(optimizer, lr_scheduler)\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:32:15.246751Z","iopub.execute_input":"2024-05-22T14:32:15.247199Z","iopub.status.idle":"2024-05-22T14:32:16.505813Z","shell.execute_reply.started":"2024-05-22T14:32:15.247169Z","shell.execute_reply":"2024-05-22T14:32:16.504625Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:32:35.871943Z","iopub.execute_input":"2024-05-22T14:32:35.872826Z","iopub.status.idle":"2024-05-22T14:32:35.887166Z","shell.execute_reply.started":"2024-05-22T14:32:35.872782Z","shell.execute_reply":"2024-05-22T14:32:35.885407Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model=trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:33:24.086180Z","iopub.execute_input":"2024-05-22T14:33:24.086623Z","iopub.status.idle":"2024-05-22T15:29:26.815378Z","shell.execute_reply.started":"2024-05-22T14:33:24.086588Z","shell.execute_reply":"2024-05-22T15:29:26.812802Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240522_143433-5h31r1ld</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ntua-antonis/huggingface/runs/5h31r1ld' target=\"_blank\">distinctive-resonance-2</a></strong> to <a href='https://wandb.ai/ntua-antonis/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ntua-antonis/huggingface' target=\"_blank\">https://wandb.ai/ntua-antonis/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ntua-antonis/huggingface/runs/5h31r1ld' target=\"_blank\">https://wandb.ai/ntua-antonis/huggingface/runs/5h31r1ld</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [57/57 53:23, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.143516</td>\n      <td>0.980000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.160022</td>\n      <td>0.976667</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.161570</td>\n      <td>0.976667</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nfrom transformers import AdamW, get_scheduler\n\n\nargs = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", per_device_train_batch_size=16)\ntraining_args = TrainingArguments(\n    output_dir=\"test_trainer\",\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=100,\n    per_device_eval_batch_size=100,\n    num_train_epochs=5,\n    weight_decay=0.01\n)\n\n# Ορισμός του optimizer\noptimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n\n# Ορισμός του scheduler\nnum_training_steps =5\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\n# Δημιουργία του Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    optimizers=(optimizer, lr_scheduler)\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T17:06:43.057062Z","iopub.execute_input":"2024-05-22T17:06:43.057775Z","iopub.status.idle":"2024-05-22T17:06:44.415623Z","shell.execute_reply.started":"2024-05-22T17:06:43.057742Z","shell.execute_reply":"2024-05-22T17:06:44.413517Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T17:07:08.737041Z","iopub.execute_input":"2024-05-22T17:07:08.737679Z","iopub.status.idle":"2024-05-22T17:07:08.748438Z","shell.execute_reply.started":"2024-05-22T17:07:08.737647Z","shell.execute_reply":"2024-05-22T17:07:08.747093Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"trained_model=trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T17:07:48.308227Z","iopub.execute_input":"2024-05-22T17:07:48.308620Z","iopub.status.idle":"2024-05-22T18:05:38.496495Z","shell.execute_reply.started":"2024-05-22T17:07:48.308591Z","shell.execute_reply":"2024-05-22T18:05:38.493594Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240522_170808-cu0c12vj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ntua-antonis/huggingface/runs/cu0c12vj' target=\"_blank\">divine-lion-3</a></strong> to <a href='https://wandb.ai/ntua-antonis/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ntua-antonis/huggingface' target=\"_blank\">https://wandb.ai/ntua-antonis/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ntua-antonis/huggingface/runs/cu0c12vj' target=\"_blank\">https://wandb.ai/ntua-antonis/huggingface/runs/cu0c12vj</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [57/57 56:01, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.143516</td>\n      <td>0.980000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.160022</td>\n      <td>0.976667</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.161570</td>\n      <td>0.976667</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nfrom transformers import AdamW, get_scheduler\n\n\nargs = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", per_device_train_batch_size=16)\ntraining_args = TrainingArguments(\n    output_dir=\"test_trainer\",\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=500,\n    per_device_eval_batch_size=500,\n    num_train_epochs=5,\n    weight_decay=0.01\n)\n\n# Ορισμός του optimizer\noptimizer = AdamW(model.parameters(), lr=0.005)\n\n# Ορισμός του scheduler\nnum_training_steps =5\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\n# Δημιουργία του Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    optimizers=(optimizer, lr_scheduler)\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:09:48.021934Z","iopub.execute_input":"2024-05-22T18:09:48.022364Z","iopub.status.idle":"2024-05-22T18:09:48.047813Z","shell.execute_reply.started":"2024-05-22T18:09:48.022334Z","shell.execute_reply":"2024-05-22T18:09:48.046776Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:10:10.516119Z","iopub.execute_input":"2024-05-22T18:10:10.516552Z","iopub.status.idle":"2024-05-22T18:10:10.531064Z","shell.execute_reply.started":"2024-05-22T18:10:10.516511Z","shell.execute_reply":"2024-05-22T18:10:10.529987Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model=trainer.train()","metadata":{"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [57/57 54:35, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.137399</td>\n      <td>0.986667</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.135277</td>\n      <td>0.986667</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.135986</td>\n      <td>0.986667</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]}]}